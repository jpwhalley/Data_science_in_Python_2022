{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling for Global Health - Data science in Python\n",
    "## Day 1: Collecting disparate data online by web scraping using Selenium and BeautifulSoup\n",
    "\n",
    "Ideally the data we wish to work on can be downloaded in an easy to use format. Otherwise when we want only a small subset of a very big dataset, or the data is being constantly updated, hopefully the owner will provide an application programming interface (API) to automate the collection of the relevant data. However quite often the data cannot be downloaded and there is no API, but the data is publicly available, just dispersed across a website. When it would be too tedious and time consuming to navigate page by page to collect the data manually; we can use Selenium Webdriver and Beautiful Soup to automate navigating across the website and collecting of the relevant data. In this code clinic, I will go through the best practices (and what not to do!) when web scraping; using Selenium Webdriver to navigate around a website and then using Beautiful Soup to extract the data from the HTML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Why automate? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Load the packages and open a remote controlled browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from selenium import webdriver\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire up Selenium webdriver with the website we want to scrape\n",
    "browser = webdriver.Firefox() # If you use a different browser, replace Firefox with this\n",
    "base_url = \"https://www.literaryclock.com/\"\n",
    "browser.get(base_url)\n",
    "\n",
    "# # Alternatively, use requests\n",
    "# requests.get(base_url, timeout=1) # Hopefully get Response [200], not [404]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Navigating around using selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Using link text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start navigating around\n",
    "element = browser.find_element_by_link_text(\"Posts\")\n",
    "print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: How to submit text, and limitations of being a robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to text, play around with inputing text\n",
    "browser.find_element_by_link_text(\"Contact\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find the boxes to input text\n",
    "element = browser.find_element_by_css_selector('input[id=\"name\"]')\n",
    "print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element.send_keys('Justin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly can you put text in the subject line\n",
    "browser.find_element_by_css_selector('input[id=\"subject\"]').send_keys('Hello!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly different challenge in inputing data into the message box\n",
    "# Message is not an 'input', it is a textarea\n",
    "browser.find_element_by_css_selector('textarea[id=\"message\"]').send_keys('How are you doing?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we automatically pretend not to be a robot, probably not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now how do we click on the send button\n",
    "browser.find_element_by_css_selector('input[id=\"submit\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Webscraping time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Can we get a the literary works, their authors and their date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to Posts\n",
    "browser.find_element_by_link_text(\"Posts\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now The Literary Clock Library\n",
    "browser.find_element_by_link_text(\"The Literary Clock Library\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use javascript to click on the element instead\n",
    "element = browser.find_element_by_link_text(\"The Literary Clock Library\")\n",
    "browser.execute_script(\"arguments[0].click();\", element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now to the 1500s\n",
    "browser.find_element_by_link_text(\"1500s\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we grab the plays first performed in the 1500s\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "html_source = browser.page_source\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "# # Alternative with requests\n",
    "# html = requests.get(base_url+'library/1500', timeout=1)\n",
    "# soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the 'li' tag\n",
    "print(soup.li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, get all the 'li' tags\n",
    "print(soup.findAll('li'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look to see if it is a link or not\n",
    "for i in soup.findAll('li'):\n",
    "    print(i.a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can filter out by link (and we can also look at the link)\n",
    "for i in soup.findAll('li'):\n",
    "    if i.a != None:\n",
    "        print(i.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = {}\n",
    "counter = 0\n",
    "for i in soup.findAll('li'):\n",
    "    if i.a == None:\n",
    "        books[counter] = {}\n",
    "        \n",
    "        book = i.string\n",
    "        \n",
    "        books[counter]['Year'] = int(book[:4])\n",
    "        \n",
    "        temp = book[6:].split(' by ')\n",
    "        \n",
    "        books[counter]['Title'] = temp[0]\n",
    "        books[counter]['Author'] = temp[1]\n",
    "        \n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can now look at this in a dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(books).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, lets add the 1600s\n",
    "browser.find_element_by_partial_link_text(\"Next\").click() # Nice alternative to find_element_by_link_text\n",
    "\n",
    "# Now put all of the books from this page into the \"books\" dictionary\n",
    "html_source = browser.page_source\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "# # Alternative with requests\n",
    "# html = requests.get(base_url+'library/1600', timeout=1)\n",
    "# soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "for i in soup.findAll('li'):\n",
    "    if i.a == None:\n",
    "        books[counter] = {}\n",
    "        \n",
    "        book = i.string\n",
    "        \n",
    "        books[counter]['Year'] = int(book[:4])\n",
    "        \n",
    "        temp = book[6:].split(' by ')\n",
    "        \n",
    "        books[counter]['Title'] = temp[0]\n",
    "        books[counter]['Author'] = temp[1]\n",
    "        \n",
    "        counter +=1\n",
    "        \n",
    "df = pd.DataFrame(books).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Automatically get all the literary works, their authors and their date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's go back to the 1500s and try get them all\n",
    "from time import sleep\n",
    "browser.find_element_by_partial_link_text(\"Prev\").click()\n",
    "\n",
    "carry_on = True\n",
    "books = {}\n",
    "counter = 0\n",
    "while carry_on:\n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    \n",
    "    for i in soup.findAll('li'):\n",
    "        if i.a == None:\n",
    "            books[counter] = {}\n",
    "        \n",
    "            book = i.string\n",
    "            # print(book)\n",
    "        \n",
    "            books[counter]['Year'] = int(book[:4])\n",
    "        \n",
    "            temp = book[6:].split(' by ')\n",
    "        \n",
    "            books[counter]['Title'] = temp[0]\n",
    "            books[counter]['Author'] = temp[1]\n",
    "        \n",
    "            counter +=1\n",
    "            \n",
    "    try:\n",
    "        browser.find_element_by_partial_link_text(\"Next\").click()\n",
    "        sleep(2)\n",
    "    except:\n",
    "        carry_on=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(books).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, lets check how best to collect the data\n",
    "# Can do it in two parts - first up 1500s, 1600s and 1700s\n",
    "\n",
    "books = {}\n",
    "counter = 0\n",
    "for century in ['/1500', '/1600', '/1700']:\n",
    "    browser.get(base_url + 'library' + century)\n",
    "    html_source = browser.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    \n",
    "#     # Alternative with requests\n",
    "#     html = requests.get(base_url + 'library' + century, timeout=1)\n",
    "#     soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    \n",
    "    for i in soup.findAll('li'):\n",
    "        if i.a == None:\n",
    "            books[counter] = {}\n",
    "\n",
    "            book = i.string\n",
    "            # print(book)\n",
    "\n",
    "            books[counter]['Year'] = int(book[:4])\n",
    "\n",
    "            temp = book[6:].split(' by ')\n",
    "\n",
    "            books[counter]['Title'] = temp[0]\n",
    "            books[counter]['Author'] = temp[1]\n",
    "\n",
    "            counter +=1\n",
    "    \n",
    "    sleep(2)\n",
    "\n",
    "df = pd.DataFrame(books).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the 1800s, 1900s and 2000s - For students to do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look to quickly plot the distribution of years\n",
    "df['Year'] = df['Year'].astype('int')\n",
    "df.hist(column='Year', bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
