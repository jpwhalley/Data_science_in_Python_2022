{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling for Global Health - Data science in Python\n",
    "## Day 3: Dimension reduction methods.\n",
    "### Plotting a map of Europe given distances between cities\n",
    "+ Distance file available from RMDS project:\n",
    "    - [https://github.com/cheind/rmds/blob/master/examples/european_city_distances.csv](https://github.com/cheind/rmds/blob/master/examples/european_city_distances.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "dists = pd.read_csv(\"european_city_distances.csv\", delimiter=';', index_col=0)\n",
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in a easy to use matrix and labels\n",
    "adist = np.array(dists)\n",
    "cities = dists.index.values\n",
    "\n",
    "# Caculate the coordinates in a 2D map\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6) # In 2D, with distances already precomputed and fixed orientation\n",
    "results = mds.fit(adist)\n",
    "\n",
    "coords = results.embedding_\n",
    "\n",
    "# Plot a 2D map of based on the distances from each other.\n",
    "with sns.axes_style(\"white\"):\n",
    "    plt.figure(1,figsize=(20,10))\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], marker = 'o', color='black', s=40)\n",
    "    texts = []\n",
    "    for label, x, y in zip(cities, coords[:, 0], coords[:, 1]):\n",
    "        texts.append(plt.text(x,y,label,size=15,color='darkblue'))\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about using genetic distances to map ethnicity\n",
    "\n",
    "+ Firstly find genome positiions in your cohort that are informative about race:\n",
    "    - Use the 1000 genome dataset to pick out positions with a minor allele frequency different between two of African, Asian or European populations (filtering out any multi allelic sites).\n",
    "+ For these sites, calculate the Euclidean distance between your samples:\n",
    "    - To calculate the distance at each position, for a pair of samples, if they are both share the same alleles, distance = 0; or share one allele, distance = 1; and if they share no alleles, distance = 2.\n",
    "    - Square each the distance at each position, add them all together and take the square root of the answer.\n",
    "    - This is used to plot a 2D map of genomic 'distance' between the samples. We can compare the reported ethnicity to the position on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in\n",
    "genetic_distances = np.genfromtxt('genetic_distances.csv', delimiter=',')\n",
    "recorded_ethnicities = open('recorded_ethnicities.txt').read().splitlines()\n",
    "\n",
    "# Define the markers and colours for each recorded ethnicity\n",
    "ethnicities = {\n",
    "'Australia': ['Australian', 'd', 'blue'], \n",
    "'Germany': ['German', 's', 'blue'], \n",
    "'Poland': ['Poland', '1', 'blue'], \n",
    "'Russia': ['Russia', 'p', 'blue'], \n",
    "'Ukraine': ['Ukraine', '3', 'blue'], \n",
    "'Yemen': ['Yemen', '4', 'red'], \n",
    "'Korea South': ['S_Korea', 'D', 'red'], \n",
    "'Singapore': ['Singapore', '|', 'red'], \n",
    "'Vietnam': ['Vietnam', '>', 'red'], \n",
    "'Canada_Wh': ['White_Canadian', '^', 'blue'], 'Canada_As': ['Asian_Canadian', '^', 'red'],  'Canada_NK': ['Unknown_Canadian', '^', 'black'], \n",
    "'USA_Af': ['African_American','o','green'], 'USA_As': ['Asian_American','o','red'], 'USA_Wh': ['White_American','o','blue'], 'USA_La': ['Latin_American','o','gold'], 'USA_NK': ['Unknown_American', 'o', 'black'], \n",
    "'Unknown_Af': ['African_Unknown','x','green'], 'Unknown_As': ['Asian_Unknown','x','red'], 'Unknown_Wh': ['White_Unknown','x','blue'], 'Unknown_La': ['Latin_Unknown','x','gold'], 'Unknown_PI': ['Pacific_Islander_Unknown','x','purple'], 'Unknown_NK': ['Unknown_Unknown', 'x', 'black']}\n",
    "\n",
    "# OK Calculation time\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6)\n",
    "results = mds.fit(genetic_distances)\n",
    "\n",
    "coords = results.embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the order to plot it in\n",
    "nationalities = []\n",
    "for item in ethnicities:\n",
    "    nationalities.append(item)\n",
    "\n",
    "plotting_dict = {}\n",
    "for item in nationalities:\n",
    "    plotting_dict[item] = []\n",
    "\n",
    "for k,item in enumerate(recorded_ethnicities):\n",
    "    plotting_dict[item].append(k)\n",
    "\n",
    "correct_order = []\n",
    "for key in plotting_dict:\n",
    "    correct_order.append(key)\n",
    "correct_order = list(set(correct_order))\n",
    "correct_order\n",
    "# correct_order.sort()\n",
    "\n",
    "# Plot a 2D map of genetic distances\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig = plt.figure(1,figsize=(18, 12))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    correct_order = ['USA_Af', 'Unknown_Af', 'Canada_As', 'USA_As', 'Korea South', 'Singapore', 'Vietnam', 'Yemen', 'Unknown_As', 'Unknown_PI', 'Australia', 'Canada_Wh', 'USA_Wh', 'Germany', 'Poland', 'Ukraine', 'Russia', 'Unknown_Wh', 'USA_La', 'Unknown_La', 'Canada_NK', 'USA_NK', 'Unknown_NK']\n",
    "    \n",
    "    for key in correct_order:\n",
    "        if key in ethnicities:\n",
    "             plt.scatter(coords[plotting_dict[key], 0], coords[plotting_dict[key], 1], c=ethnicities[key][2], marker=ethnicities[key][1],label=ethnicities[key][0])\n",
    "    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * .8, box.height])\n",
    "    ax.legend(loc = 'center left', fontsize='xx-large',bbox_to_anchor=(1, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with gene expression data\n",
    "## Looking at dimension reduction, using principal component analysis\n",
    "\n",
    "In this dataset we have:\n",
    "    - 228 donors, ~15000 gene expression values\n",
    "\n",
    "The gene expression is from the innate immune cells, monocytes under 4 conditions:\n",
    "    - Untreated, as a control.\n",
    "    - Treated with inteferon-gamma (IFN) for 24 hours â€“ a good model for viral infections.\n",
    "    - Treated with Lipopolysaccharide (LPS) for 2 hours - LPS is a major component of the outer wall of gram negative bacteria, which our body registers as a toxin and elicits a strong immune response.\n",
    "    - Treated with Lipopolysaccharide (LPS) for 24 hours.\n",
    "    \n",
    "So we have a dataset for 912 samples (from 228 donors for 4 conditions each), gene expression data for ~15,000 genes. \n",
    "\n",
    "How to understand this dataset?\n",
    "\n",
    "No doubt there is high redundancy amongst the samples, so reducing them from ~15000 to a smaller number could be really helpful into interpreting the dataset (in this case for projecting the gene expression and genes into one value for each sample for each principal component).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in\n",
    "expr_all = pd.read_csv('monocyte_all_expression.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what it looks like\n",
    "expr_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the treatment for each sample\n",
    "y = []\n",
    "for item in expr_all.index.values:\n",
    "    if 'Untreated' in item:\n",
    "        y.append(0)\n",
    "    elif 'IFN' in item:\n",
    "        y.append(1)\n",
    "    elif 'LPS2h' in item:\n",
    "        y.append(2)\n",
    "    elif 'LPS24h' in item:\n",
    "        y.append(3)\n",
    "    else:\n",
    "        print('Error: Data from non-recognisable experiment')\n",
    "y = np.array(y)\n",
    "\n",
    "labels = ['Untreated', 'IFN', 'LPS 2h', 'LPS 24h']\n",
    "colours = ['red', 'green', 'blue', 'gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of component 1 and 2\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(expr_all)\n",
    "var_expl = pca.explained_variance_ratio_\n",
    "r_col = []\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(colours, range(len(labels)), labels):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('1st Comp: ' + str(round(var_expl[0]*100,1)) + '% variance explained')\n",
    "plt.ylabel('2nd Comp: ' + str(round(var_expl[1]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp1_comp2.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of component 2 and 3\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(colours, range(len(labels)), labels):\n",
    "    plt.scatter(X_pca[y == i, 1], X_pca[y == i, 2], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('2nd Comp: ' + str(round(var_expl[1]*100,1)) + '% variance explained')\n",
    "plt.ylabel('3rd Comp: ' + str(round(var_expl[2]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp2_comp3.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of component 3 and 4\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(colours, range(len(labels)), labels):\n",
    "    plt.scatter(X_pca[y == i, 2], X_pca[y == i, 4], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('3rd Comp: ' + str(round(var_expl[2]*100,1)) + '% variance explained')\n",
    "plt.ylabel('4th Comp: ' + str(round(var_expl[3]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp2_comp3.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data\n",
    "\n",
    "From the PCA (well for the first-  three components), we can see a good separation between the four different treatments of the cells. Can we use clustering methods to fully classify them?\n",
    "\n",
    "Read through this tutorial - https://realpython.com/k-means-clustering-python/ and have a go at clustering the PCA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying on th PCA, can we use clustering methods on all the data to fully classify them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
