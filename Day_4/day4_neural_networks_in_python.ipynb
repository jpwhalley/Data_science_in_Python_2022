{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling for Global Health - Data science in Python\n",
    "## Day 4: Neural networks in Python\n",
    "\n",
    "Carrying on from yesterday, we will look to use the feature-rich, gene expression data to practise classification using neural networks. Can we acurrately classify the samples to the experimental conditions based on their gene expression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for importing, cleaning and looking at the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from natsort import index_natsorted, order_by_index\n",
    "import random\n",
    "from collections import Counter\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "expr_new_info = pd.read_csv('Common_info_data.txt', index_col=0)\n",
    "\n",
    "expr_naive = pd.read_csv('Common_naive_data.txt', index_col=0)\n",
    "expr_ifn = pd.read_csv('Common_ifn_data.txt', index_col=0)\n",
    "expr_lps2 = pd.read_csv('Common_lps2_data.txt', index_col=0)\n",
    "expr_lps24 = pd.read_csv('Common_lps24_data.txt', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Discover the dataset\n",
    "for expr in [expr_naive, expr_ifn, expr_lps2, expr_lps24]:\n",
    "    print(expr.shape)\n",
    "    print(expr.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normalizing the data\n",
    "def initial_normalize(df):\n",
    "    result = df.copy()\n",
    "    feature_max = {}\n",
    "    feature_min = {}\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        feature_max[feature_name] = max_value + 0.1\n",
    "        min_value = df[feature_name].min() - 0.1\n",
    "        feature_min[feature_name] = min_value\n",
    "        if max_value == 0:\n",
    "            result[feature_name]= df[feature_name]\n",
    "        else:\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result, feature_max, feature_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the dataset and normalize it\n",
    "samps_in_common = list(set(expr_naive.columns.values) & set(expr_ifn.columns.values) & set(expr_lps2.columns.values) & set(expr_lps24.columns.values))\n",
    "\n",
    "expr_all_treat = pd.concat([expr_naive[samps_in_common].T,expr_ifn[samps_in_common].T,expr_lps24[samps_in_common].T,expr_lps2[samps_in_common].T], keys=['Naive', 'IFN', 'LPS24', 'LPS2'])\n",
    "expr_all_treat_norm, feature_max, feature_min = initial_normalize(expr_all_treat)\n",
    "classes = ['Naive', 'IFN', 'LPS24', 'LPS2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training and testing sets\n",
    "rows = random.sample(range(len(expr_all_treat_norm.index)), int(.75*len(expr_all_treat_norm.index)))\n",
    "rows.sort()\n",
    "\n",
    "training = expr_all_treat_norm.values[rows,]\n",
    "training_labels = np.array(expr_all_treat_norm.index.get_level_values(0)[rows])\n",
    "testing = np.delete(expr_all_treat_norm.values,rows,axis=0)\n",
    "testing_labels = np.array(np.delete(expr_all_treat_norm.index.get_level_values(0), [rows], axis=0))\n",
    "\n",
    "training_samples = Counter(training_labels)\n",
    "print('For training set we have the following samples:')\n",
    "for key in training_samples:\n",
    "    print(key, training_samples[key])\n",
    "\n",
    "testing_samples = Counter(testing_labels)\n",
    "print('For testing set we have the following samples:')\n",
    "for key in testing_samples:\n",
    "    print(key, testing_samples[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocess the data\n",
    "scaled_training = training\n",
    "scaled_testing = testing\n",
    "\n",
    "scaled_training_labels = np.zeros((len(training),len(classes)))\n",
    "for i,tr in enumerate(training_labels):\n",
    "    scaled_training_labels[i,classes.index(tr)] = 1\n",
    "scaled_testing_labels = np.zeros((len(testing),len(classes)))\n",
    "for i,tr in enumerate(testing_labels):\n",
    "    scaled_testing_labels[i,classes.index(tr)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "x_train = scaled_training\n",
    "y_train = scaled_training_labels\n",
    "\n",
    "x_test = scaled_testing\n",
    "y_test = scaled_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=17867, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2'))\n",
    "model.add(Dense(50, activation='relu', name='layer_3'))\n",
    "model.add(Dense(4, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_rate = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure_dense.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights_dense.h5\")\n",
    "\n",
    "# # Load the json file that contains the model's structure\n",
    "# f = Path(\"data/ks2a_model_structure_dense.json\")\n",
    "# model_structure = f.read_text()\n",
    "#\n",
    "# # Recreate the Keras model object from the json data\n",
    "# model = model_from_json(model_structure)\n",
    "#\n",
    "# # Re-load the model's trained weights\n",
    "# model.load_weights(\"data/ks2a_model_weights_dense.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict non-matched samples\n",
    "# Remove samples already used in making the model\n",
    "expr_naive_p = expr_naive.drop(samps_in_common,axis=1).T\n",
    "expr_ifn_p = expr_ifn.drop(samps_in_common, axis=1).T\n",
    "expr_lps24_p = expr_lps24.drop(samps_in_common, axis=1).T\n",
    "expr_lps2_p = expr_lps2.drop(samps_in_common, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For normalizing the data the same way as before\n",
    "def subsidary_normalize(df, feature_max, feature_min):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = feature_max[feature_name]\n",
    "        min_value = feature_min[feature_name]\n",
    "        if max_value == 0:\n",
    "            result[feature_name]= df[feature_name]\n",
    "        else:\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_all_eval = pd.concat([expr_naive_p, expr_ifn_p, expr_lps24_p, expr_lps2_p], keys=['Naive', 'IFN', 'LPS24', 'LPS2'])\n",
    "expr_all_eval_norm = subsidary_normalize(expr_all_eval, feature_max, feature_min)\n",
    "evaluation = expr_all_eval_norm.values\n",
    "evaluation_labels = np.array(expr_all_eval_norm.index.get_level_values(0))\n",
    "\n",
    "evaluation_samples = Counter(evaluation_labels)\n",
    "print('For evaluation set we have the following samples:')\n",
    "for key in evaluation_samples:\n",
    "    print(key, evaluation_samples[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_evaluation = evaluation\n",
    "\n",
    "scaled_evaluation_labels = np.zeros((len(evaluation),len(classes)))\n",
    "for i,tr in enumerate(evaluation_labels):\n",
    "    scaled_evaluation_labels[i,classes.index(tr)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_evaluation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the neural network\n",
    "X = scaled_evaluation\n",
    "prediction = model.predict(X)\n",
    "df_pred = pd.DataFrame(data=prediction, index=expr_all_eval.index, columns=['Naive', 'IFN', 'LPS24', 'LPS2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Predict the new rna-seq samples\n",
    "classes = ['IFNG', 'LPS2', 'LPS24', 'LPS6', 'UT']\n",
    "count_data_common = pd.read_csv('Common_count_data.txt', index_col=0)\n",
    "rna_data = count_data_common.T\n",
    "rna_data['sample'] = [x.split(\"_\")[0] for x in rna_data.index]\n",
    "rna_data['treatment'] = [x.split(\"_\")[1] for x in rna_data.index]\n",
    "rna_data = rna_data.set_index(['treatment', 'sample'])\n",
    "rna_data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_eval, _, _ = initial_normalize(rna_data)\n",
    "rna_evaluation = rna_eval.values\n",
    "rna_evaluation_labels = np.array(rna_eval.index.get_level_values(0))\n",
    "\n",
    "rna_evaluation_samples = Counter(rna_evaluation_labels)\n",
    "print('For RNA evaluation set we have the following samples:')\n",
    "for key in rna_evaluation_samples:\n",
    "    print(key, rna_evaluation_samples[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_scaled_evaluation = rna_evaluation\n",
    "\n",
    "rna_scaled_evaluation_labels = np.zeros((len(rna_evaluation),len(classes)))\n",
    "for i,tr in enumerate(rna_evaluation_labels):\n",
    "    rna_scaled_evaluation_labels[i,classes.index(tr)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the neural network\n",
    "X = rna_scaled_evaluation\n",
    "rna_prediction = model.predict(X)\n",
    "df_rna_pred = pd.DataFrame(data=rna_prediction, index=rna_data.index, columns=['Naive', 'IFN', 'LPS24', 'LPS2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rna_pred.style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different neural network models\n",
    "### Dense layers, no loss\n",
    "So far we have used one of the simplest Neural Networks with dense layers and no loss, though it possible that we are over fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=17867, activation='relu', name='layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='layer_2'))\n",
    "model.add(Dense(50, activation='relu', name='layer_3'))\n",
    "model.add(Dense(4, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layers, with loss\n",
    "One way to avoid overfitting, is to randomly add loss to the network, so not all the data makes it from layer to layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model with loss\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=17867, activation='relu', name='layer_1'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(100, activation='relu', name='layer_2'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(50, activation='relu', name='layer_3'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "# # Save neural network structure\n",
    "# model_structure = model.to_json()\n",
    "# f = Path(\"model_structure_loss.json\")\n",
    "# f.write_text(model_structure)\n",
    "\n",
    "# # Save neural network's trained weights\n",
    "# model.save_weights(\"model_weights_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_rate = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure_loss.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights_loss.h5\")\n",
    "\n",
    "# # Load the json file that contains the model's structure\n",
    "# f = Path(\"data/ks2b_model_structure_loss.json\")\n",
    "# model_structure = f.read_text()\n",
    "#\n",
    "# # Recreate the Keras model object from the json data\n",
    "# model = model_from_json(model_structure)\n",
    "#\n",
    "# # Re-load the model's trained weights\n",
    "# model.load_weights(\"ks2b_model_weights_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the neural network - non-matched samples\n",
    "X = scaled_evaluation\n",
    "prediction = model.predict(X)\n",
    "df_pred = pd.DataFrame(data=prediction, index=expr_all_eval.index, columns=['Naive', 'IFN', 'LPS24', 'LPS2'])\n",
    "df_pred.style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the neural network\n",
    "X = rna_scaled_evaluation\n",
    "rna_prediction = model.predict(X)\n",
    "df_rna_pred = pd.DataFrame(data=rna_prediction, index=rna_data.index, columns=['Naive', 'IFN', 'LPS24', 'LPS2'])\n",
    "df_rna_pred.style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer with loss\n",
    "Loss just on dense layers has too much of a detrimental effect on our results. How about if we segment our data, but keep the loss between layers in?\n",
    "Note the input of this data into a convolution layer needs an extra dimension, which then needs to be pooled and flattened before beingn put into a dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "x_train = np.expand_dims(scaled_training, axis=2)\n",
    "y_train = scaled_training_labels\n",
    "\n",
    "x_test = np.expand_dims(scaled_testing, axis=2)\n",
    "y_test = scaled_testing_labels\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, 10, input_shape=(17867,1), padding='same', activation=\"relu\"))\n",
    "model.add(MaxPooling1D(200))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_rate = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure_conv_10.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights_conv_10.h5\")\n",
    "\n",
    "# # Load the json file that contains the model's structure\n",
    "# f = Path(\"data/ks2c_model_structure_conv_10.json\")\n",
    "# model_structure = f.read_text()\n",
    "#\n",
    "# # Recreate the Keras model object from the json data\n",
    "# model = model_from_json(model_structure)\n",
    "#\n",
    "# # Re-load the model's trained weights\n",
    "# model.load_weights(\"data/ks2c_model_weights_conv_10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the neural network\n",
    "X = np.expand_dims(rna_scaled_evaluation, axis=2)\n",
    "rna_prediction = model.predict(X)\n",
    "df_rna_pred = pd.DataFrame(data=rna_prediction, index=rna_data.index, columns=['Naive', 'IFN', 'LPS24', 'LPS2'])\n",
    "df_rna_pred.style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
